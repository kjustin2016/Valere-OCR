import os
import boto3
import json
from dotenv import load_dotenv
from textractcaller.t_call import call_textract
from textractprettyprinter.t_pretty_print import (Textract_Pretty_Print, get_string)
import trp.trp2 as t2
from tabulate import tabulate

index_1 = 34

load_dotenv()

aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_region = os.getenv("AWS_REGION")

s3 = boto3.client('s3',
                  aws_access_key_id=aws_access_key_id,
                  aws_secret_access_key=aws_secret_access_key)

textract = boto3.client('textract',
                        aws_access_key_id=aws_access_key_id,
                        aws_secret_access_key=aws_secret_access_key,
                        region_name=aws_region)

def get_s3_bucket_object_by_index(bucket, index=index_1):
    if index is None:  # Check if the index is None
        return None  # Skip the array index search and directly go to the entity tag

    paginator = s3.get_paginator('list_objects_v2')  # Paginate through the bucket
    operation_parameters = {'Bucket': bucket}
    page_iterator = paginator.paginate(**operation_parameters)

    # Loop through all pages and objects
    all_object_keys = []  # Store all object keys here for debugging purposes
    for page in page_iterator:
        if 'Contents' in page and page['Contents']:
            object_keys = [item['Key'] for item in page['Contents']]
            all_object_keys.extend(object_keys)  # Collect all object keys

    # Check if the index is valid
    if len(all_object_keys) > index:
        return all_object_keys[index]  # Return the object key at the given index
    else:
        print(f"Index {index} is out of range. Total objects available: {len(all_object_keys)}")
    return None  # Return None if index is out of bounds

# Function to split queries into batches of 15
def split_queries(queries, batch_size=15):
    for i in range(0, len(queries), batch_size):
        yield queries[i:i + batch_size]

# Define all queries
all_queries = [
    {"Text": "What is the Member Name", "Alias": "MEMBER_NAME"},
    {"Text": "What is the Member ID?", "Alias": "MEMBER_ID"},
    {"Text": "Who is the PCP?", "Alias": "PCP"},
    {"Text": "What is the phone number of the PCP?", "Alias": "PCP_PHONE"},
    {"Text": "What is medical insurance provider?", "Alias": "MEDICAL_PROVIDER"},
    {"Text": "What is the effective date?", "Alias": "EFFECTIVE_DATE"},
    {"Text": "What is the Group No.?", "Alias": "GROUP_NUMBER"},
    {"Text": "What is the plan type?", "Alias": "PLAN_TYPE"},
    {"Text": "What is the BIN?", "Alias": "BIN"},
    {"Text": "What is the Rx PCN?", "Alias": "RX-PCN"},
    {"Text": "What is the Generic Copay?", "Alias": "GENERIC_COPAY"},
    {"Text": "What is the Brand Copay?", "Alias": "BRAND_COPAY"},
    {"Text": "What is the Rx Specialty Copay?", "Alias": "SPECIALTY_COPAY"},
    {"Text": "What is the Emergency Room Percentage?", "Alias": "EMERGENCY_ROOM_PERCENTAGE"},
    {"Text": "What is the PCP Copay?", "Alias": "PCP_COPAY"},
    {"Text": "What is the Specialist Copay?", "Alias": "SPECIALIST_COPAY"}
]

# Split queries into batches of 15
query_batches = list(split_queries(all_queries, batch_size=15))

# Process each batch of queries
all_query_answers = []  # To store all query answers
processed_queries = set()  # To track processed queries and avoid duplicates

for batch in query_batches:
    response = textract.analyze_document(
        Document={'S3Object': {'Bucket': 'capstone-intelligent-document-processing', 'Name': get_s3_bucket_object_by_index('capstone-intelligent-document-processing', index_1)}},
        FeatureTypes=["QUERIES"],
        QueriesConfig={"Queries": batch}
    )

    # Load the response into TDocumentSchema
    d = t2.TDocumentSchema().load(response)
    page = d.pages[0]

    # Get query answers for the current batch
    query_answers = d.get_query_answers(page=page)

    # Append unique answers to the combined list
    for x in query_answers:
        # Use a stricter key to ensure uniqueness
        query_key = (x[0], x[1], x[2])  # Include Query Text, Alias, and Answer
        if query_key not in processed_queries:
            all_query_answers.append(x)
            processed_queries.add(query_key)

    # Print query answers for the current batch
    for x in query_answers:
        print(f"{get_s3_bucket_object_by_index('capstone-intelligent-document-processing', index_1)}, {x[1]}, {x[2]}")

# Create a mapping of query aliases to their positions in the all_queries array
query_order = {query["Alias"]: index for index, query in enumerate(all_queries)}

# Sort the all_query_answers list based on the order in the all_queries array
all_query_answers_sorted = sorted(all_query_answers, key=lambda x: query_order.get(x[1], float('inf')))

# Print the sorted query answers in a table format
print(tabulate(all_query_answers_sorted, headers=["Query", "Alias", "Answer"], tablefmt="github"))
